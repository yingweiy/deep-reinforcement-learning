{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from parallelEnv import parallelEnv\n",
    "import pong_utils\n",
    "from torch.distributions import Categorical\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, envs):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy = Policy().to(self.device)\n",
    "        self.envs = envs\n",
    "        self.n_actions = self.policy.n_actions\n",
    "        self.tmax = 320\n",
    "\n",
    "    def clipped_surrogate(self, old_probs, states, actions, rewards,\n",
    "                          discount=0.995, epsilon=0.1, beta=0.01):\n",
    "        discount = discount ** np.arange(len(rewards))\n",
    "        rewards = np.asarray(rewards) * discount[:, np.newaxis]\n",
    "\n",
    "        # convert rewards to future rewards\n",
    "        rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "\n",
    "        mean = np.mean(rewards_future, axis=1)\n",
    "        std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "        rewards_normalized = (rewards_future - mean[:, np.newaxis]) / std[:, np.newaxis]\n",
    "\n",
    "        # convert everything into pytorch tensors and move to gpu if available\n",
    "        actions = torch.tensor(actions, dtype=torch.int64, device=self.device)\n",
    "        old_probs = torch.stack(old_probs).squeeze(2)\n",
    "\n",
    "        rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=self.device)\n",
    "\n",
    "        # convert states to policy (or probability)\n",
    "        new_probs = self.states_to_prob(states, actions)\n",
    "\n",
    "        # ratio for clipping\n",
    "        ratio = new_probs / old_probs\n",
    "\n",
    "        # clipped function\n",
    "        clip = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "        clipped_surrogate = torch.min(ratio * rewards, clip * rewards)\n",
    "\n",
    "        # include a regularization term\n",
    "        # this steers new_policy towards 0.5\n",
    "        # add in 1.e-10 to avoid log(0) which gives nan\n",
    "        entropy = -(new_probs * torch.log(old_probs + 1.e-10) + \\\n",
    "                    (1.0 - new_probs) * torch.log(1.0 - old_probs + 1.e-10))\n",
    "\n",
    "        # this returns an average of all the entries of the tensor\n",
    "        # effective computing L_sur^clip / T\n",
    "        # averaged over time-step and number of trajectories\n",
    "        # this is desirable because we have normalized our rewards\n",
    "        return torch.mean(clipped_surrogate + beta * entropy)\n",
    "\n",
    "\n",
    "    def train(self, episode=800, discount_rate = 0.99, epsilon = 0.1, beta=0.01,\n",
    "              SGD_epoch = 4, lr=1e-4):\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        # keep track of progress\n",
    "        mean_rewards = []\n",
    "\n",
    "        for e in tqdm(range(episode)):\n",
    "            # collect trajectories\n",
    "            old_probs, states, actions, rewards = \\\n",
    "                self.collect_trajectories(tmax=self.tmax)\n",
    "\n",
    "            total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "            # gradient ascent step\n",
    "            for _ in range(SGD_epoch):\n",
    "                # uncomment to utilize your own clipped function!\n",
    "                # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "                L = -self.clipped_surrogate(old_probs, states, actions, rewards,\n",
    "                                       epsilon=epsilon, beta=beta)\n",
    "                self.optimizer.zero_grad()\n",
    "                L.backward(retain_graph=False)\n",
    "                self.optimizer.step()\n",
    "                del L\n",
    "\n",
    "            # the clipping parameter reduces as time goes on\n",
    "            epsilon *= .999\n",
    "\n",
    "            # the regulation term also reduces\n",
    "            # this reduces exploration in later runs\n",
    "            beta *= .995\n",
    "\n",
    "            # get the average reward of the parallel environments\n",
    "            mean_rewards.append(np.mean(total_rewards))\n",
    "\n",
    "            # display some progress every 20 iterations\n",
    "            if (e + 1) % 20 == 0:\n",
    "                print(\"Episode: {0:d}, score: {1:f}\".format(e + 1, np.mean(total_rewards)))\n",
    "                print(total_rewards)\n",
    "\n",
    "\n",
    "    def states_to_prob(self, states, actions):\n",
    "        statesv = torch.stack(states)\n",
    "        policy_input = statesv.view(-1, *statesv.shape[-3:])\n",
    "        policy_output = self.policy(policy_input).view([self.tmax, -1, self.n_actions]) #t_max, n_workers, n_actions\n",
    "        probs = torch.gather(policy_output, 2, actions).squeeze(2)\n",
    "        return probs\n",
    "\n",
    "    # collect trajectories for a parallelized parallelEnv object\n",
    "    def collect_trajectories(self, tmax, nrand=5):\n",
    "\n",
    "        # number of parallel instances\n",
    "        n = len(self.envs.ps)\n",
    "\n",
    "        # initialize returning lists and start the game!\n",
    "        state_list = []\n",
    "        reward_list = []\n",
    "        prob_list = []\n",
    "        action_list = []\n",
    "\n",
    "        self.envs.reset()\n",
    "\n",
    "        # start all parallel agents\n",
    "        self.envs.step([1] * n)\n",
    "\n",
    "        # perform nrand random steps\n",
    "        for _ in range(nrand):\n",
    "            fr1, re1, _, _ = self.envs.step(np.random.choice([pong_utils.RIGHT, pong_utils.LEFT], n))\n",
    "            fr2, re2, _, _ = self.envs.step([0] * n)\n",
    "\n",
    "        for t in range(tmax):\n",
    "\n",
    "            # prepare the input\n",
    "            # preprocess_batch properly converts two frames into\n",
    "            # shape (n, 2, 80, 80), the proper input for the policy\n",
    "            # this is required when building CNN with pytorch\n",
    "            batch_input = pong_utils.preprocess_batch([fr1, fr2])\n",
    "\n",
    "            # probs will only be used as the pi_old\n",
    "            # no gradient propagation is needed\n",
    "            # so we move it to the cpu\n",
    "            probs_tensor = self.policy(batch_input).detach()\n",
    "            m = Categorical(probs_tensor)\n",
    "            action = m.sample().unsqueeze(1)\n",
    "            probs = torch.gather(probs_tensor, 1, action)\n",
    "            action = action.cpu().numpy()\n",
    "            \n",
    "            # advance the game (0=no action)a\n",
    "            # we take one action and skip game forward\n",
    "            fr1, re1, is_done, _ = self.envs.step(action+4)\n",
    "            fr2, re2, is_done, _ = self.envs.step([0] * n)\n",
    "\n",
    "            reward = re1 + re2\n",
    "\n",
    "            # store the result\n",
    "            state_list.append(batch_input)\n",
    "            reward_list.append(reward)\n",
    "            prob_list.append(probs)\n",
    "            action_list.append(action)\n",
    "\n",
    "            # stop if any of the trajectories is done\n",
    "            # we want all the lists to be retangular\n",
    "            if is_done.any():\n",
    "                break\n",
    "\n",
    "        # return pi_theta, states, actions, rewards, probability\n",
    "        return prob_list, state_list, \\\n",
    "               action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, n_actions = 2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        # 80x80x2 to 38x38x4\n",
    "        # 2 channel from the stacked frame\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 38x38x4 to 9x9x32\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size = 9 * 9 * 16\n",
    "\n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Process Process-8:\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process Process-4:\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "Process Process-5:\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/parallelEnv.py\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/yyu17/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(envs=envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 20/800 [01:23<54:24,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, score: -12.875000\n",
      "[-17. -12. -15. -17. -16.  -3. -12. -11.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 40/800 [02:45<52:33,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, score: -13.750000\n",
      "[-14. -15.  -9. -15. -14. -11. -15. -17.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 44/800 [03:02<52:13,  4.14s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3d92c9b1ff97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-90d7556a1b4c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episode, discount_rate, epsilon, beta, SGD_epoch, lr)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# collect trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mold_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-90d7556a1b4c>\u001b[0m in \u001b[0;36mcollect_trajectories\u001b[0;34m(self, tmax, nrand)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# shape (n, 2, 80, 80), the proper input for the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# this is required when building CNN with pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mbatch_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# probs will only be used as the pi_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yyu17/work/deep-reinforcement-learning/Pong-PPO/pong_utils.py\u001b[0m in \u001b[0;36mpreprocess_batch\u001b[0;34m(images, bkg_color)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# subtract bkg and crop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color,\n\u001b[0;32m---> 32\u001b[0;31m                                     axis=-1)/255.\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mbatch_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_images_prepro\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yyu17/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2919\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2920\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yyu17/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         ret = um.true_divide(\n\u001b[0;32m---> 78\u001b[0;31m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
